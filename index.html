<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving"
    />
    <meta
      name="keywords"
      content="VLM, spatial reasoning, spatial understanding, Large Language Model, LLM, multimodal, visual language model, VLM, vision=language model, distance estimation"
    />
    <meta
      property="og:image"
      content="TODO"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
        NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving
    </title>

    <!--TWITTER TODO-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Spatial Visual Language Model" />
    <meta
      name="twitter:description"
      content="NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving"
    />
    <meta
      name="twitter:image"
      content="TODO"
    />

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="./static/images/favicon.svg" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark
                for Vision-Language Models in Autonomous Driving
              </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  Kexin Tian</a><sup>1</sup>,
                </span>      
                <span class="author-block">
                  Jingrui Mao</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  Yunlong Zhang</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  Jiwan Jiang</a
                  ><sup>2</sup>,</span
                >
                <span class="author-block">
                  Yang Zhou</a
                  ><sup>1,*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://vztu.github.io/">Zhengzhong Tu</a
                  ><sup>1,*</sup>
                </span>
              </div>
              <div>
                <sup>*</sup> Corresponding authors.
              </div>

              <br />
              <div class="is-size-5 publication-authors">
                <ul class="affiliation-list">
                  <li class="affiliation">
                    <img
                      src="./static/images/tamu.jpg"
                      alt="TAMU Logo"
                      class="logo"
                    />
                    <span class="author-block"
                      ><sup>1</sup>Texas A&M University</span
                    >
                  </li>
                  <li class="affiliation">
                    <img
                      src="./static/images/uwmadison.png"
                      alt="UW-MADISON Logo"
                      class="logo"
                    />
                    <span class="author-block"><sup>2</sup>University of Wisconsin-Madison</span>
                  </li>
                </ul>
                <br>
              </div>
              <br />
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a
                      href="[TODO]"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <!-- Video Link. -->
                  <span class="link-block">
                    <a
                      href="[TODO]"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="[TODO]"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Benchmark</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://github.com/taco-group/NuScenes-SpatialQA"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
                Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning—key capabilities for autonomous driving—still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we propose NuScenes-SpatialQA, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMs' performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning.
              <!--/ Abstract. -->
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">
              Limitations of multimodal LLMs
            </h2>
            <div class="content has-text-centered">
              <img
                src="./static/images/teaser.png"
                class="inline-figure-six"
                alt="Humans can perform spatial reasoning while VLMs doesn't."
              />
            </div>
            <div class="content has-text-justified">
              <p>
                <b>Motivation:</b> Humans effortlessly determine spatial
                relationships, such as the positioning of objects relative to
                each other or estimating distances and sizes. This natural
                proficiency in direct spatial reasoning tasks contrasts with the
                current limitations of VLMs. Can we imbue VLMs with spatial
                reasoning abilities akin to those of humans?
              </p>
            </div>
            <h3 class="title is-4"></h3>
            <div class="content has-text-justified">
              <p>
                <b>Key insight:</b> We hypothesize that the limited the spatial
                reasoning abilities of current VLMs is not due to a fundamental
                limitation of their architecture, but rather is a limitation in
                common datasets available at scale on which such models are
                trained. We co-train a multimodal large language model on
                synthetic spatial data to investigate this hypothesis.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Method</h2>

            <h3 class="title is-4">Data Synthesis</h3>
            <div class="content has-text-justified">
              <p>
                We develop an automatic 3D spatial VQA data generation framework
                that lifts 2D images into metric scale 3d point clouds. We
                scales the data pipeline up to 2 billion VQA examples on 10
                million real-world images.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/method.jpg"
                class="inline-figure-six"
                alt="data synthesis pipeline"
              />
            </div>
            <h3 class="title is-4">Learning Direct Spatial Reasoning</h3>
            <div class="content has-text-justified">
              <p>
                We then mix the synthesized data into the training set of a
                multimodal large language model to train Spatial VLM. Such data
                allows the model to answer intuitive spatial reasoning questions
                such as the ones listed in the figure below. These elemental
                abilities serves as the building block for more complex spatial
                reasoning tasks such as those require multiple steps.
              </p>
              <p>
                In the figure below, we listed some sample question & answer
                pairs generated by our pipeline.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/dataset_sample.jpg"
                class="inline-figure-six"
                alt="Samples of the dataset."
              />
            </div>
            <h3 class="title is-4">Chain-of-thought Spatial Reasoning</h3>
            <div class="content has-text-justified">
              <p>
                With the ability to perform direct spatial reasoning like
                humans, we can let SpatialVLM perform Chain-of-Thought Spatial
                reasoning by letting it talk with with an LLM. As we will show
                later in experiments section, the direct reasoning capabilities,
                when combined with chain-of-thought reasoning can answer many
                multi-step questions.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Experiments</h2>

            <div class="content has-text-justified">
              <p>
                Through extensive benchmark, we found our proposed framework can
                significantly enhance the ability of visual language models in
                performing different types of spatial reasoning like humans, as
                well as unlocking novel downstream applications such as
                robotics.
              </p>
            </div>
            <h3 class="title is-4">Spatial VQA</h3>

            <div class="content has-text-justified">
              <p>
                When prompted to answer free-form binary predicate prediction
                question, such as which object is closer to the viewer,
                SpatialVLM outperforms baselines on by a large margin owing to
                the addition of synthetic data.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/qualitative_qa_samples.jpg"
                class="inline-figure-six"
                alt="qualitative spatial QA samples."
              />
              <img
                src="./static/images/qualitative_table.png"
                class="inline-figure-six"
                alt="table for comparison with baselines"
                height="auto"
                width="800px"
              />
            </div>

            <div class="content has-text-justified">
              <p>
                When finetuned with unfreezed image encoder, SpatialVLM can be
                prompted to answer quantitative spatial estimation question,
                such as the horizontal distances between objects. In particular,
                SpatialVLM outputs valid format more often than baseline methods
                when prompted to. In addition, SpatialVLM outputs quantitative
                distance estimation that is closer to ground truth annotated by
                human more often than baseline methods, with 37.2% of its
                answers falling with in 0.5x-2x range of the ground truth.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/quantitative_qa_samples.jpg"
                class="inline-figure-six"
                alt="qualitative spatial QA samples."
              />
              <img
                src="./static/images/quantitative_table.png"
                class="inline-figure-six"
                alt="Spatial VLM is able to perform quantitative spatial reasoning compared to baselines."
                height="auto"
                width="800px"
              />
            </div>

            <h3 class="title is-4">Multi-step Spatial Reasoning</h3>
            <div class="content has-text-justified">
              <p>
                In this example, with the help of an LLM orchestrating Spatial
                VLM,the system is able to answer questions like “Does the blue
                coke can, the red coke can, and the greensponge on the table
                roughly form an equilateral triangle". This opens up future
                opportunities to generate more complex spatial reasoning
                questions and answers to train a unified multimodal large
                lagnuage model.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/cot.jpg"
                class="inline-figure-six"
                alt="chain of thought spatial reasoning"
              />
            </div>

            <h3 class="title is-4">Robotics</h3>
            <div class="content has-text-justified">
              <p>
                Due to its ability to intuitively reason about space
                quantiatively in real-world units, SpatialVLM can be used as a
                fine-grained reward-annotator for robotics tasks. In the figure
                below, SpatialVLM correctly assigns a monotonically decreasing
                distance estimation for an robot hand approaching a coke can,
                which can be used as a reward signal for reinforcement learning.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/fractal_move_curve.jpg"
                class="inline-figure-four-thirds"
                alt="Comparison with baselines."
              />
            </div>
            <div class="content has-text-justified">
              <p>
                In the figure below, we show SpatialVLM can be prompted to
                annotate dense rewards for open-vocabulary robotic tasks, unlike
                many prior methods that can only annotate a binary label of
                success or failure.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/reward_heatmap.jpg"
                class="inline-figure-four-thirds"
                alt="reward heatmap."
              />
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="community-implementation">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Code (3rd Party Community Implementation)</h2>
            <div class="content has-text-justified">
              <p>
                After releasing this paper, we were greeted with enthusiasm by the VLM research community. 
                A shout-out to a user named remyxai for providing an open-source implementation of the data synthesis pipeline that closely follows our method. 
                Check it out at: <a href="https://github.com/remyxai/VQASynth"> https://github.com/remyxai/VQASynth </a>

                <div class="content has-text-centered">
                  <a href="https://github.com/remyxai/VQASynth"> 
                  <img
                  src="https://raw.githubusercontent.com/remyxai/VQASynth/main/assets/VQASynth-diagram.png
                    "
                    class="inline-figure-four-thirds"
                    alt="Comparison with baselines."
                  />
                </a>
                </div>


              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Acknowledgement</h2>
            <div class="content has-text-justified">
              <p>
                Special thanks Ying Xu and Chuyuan Kelly Fu for their help in
                creating evaluation dataset, and thank Andy Zeng and Vincent
                Vanhoucke for feedbacks on early drafts of this paper. Thanks to remyxai for providing an open-source implementation of the data synthesis pipeline.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title has-text-centered">BibTeX</h2>
        <pre><code>@article{chen2024spatialvlm,
  title = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
  author = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  journal = {arXiv preprint arXiv:2401.12168},
  year = {2024},
  url = {https://arxiv.org/abs/2401.12168},
}</code></pre>

<p> If you are using the open source implementation listed above, please also cite:</p>

      <pre><code>@misc{VQASynth,
        author = {remyxai},
        title = {VQASynth},
        year = {2024},
        note = {GitHub repository},
        url = {https://github.com/remyxai/VQASynth/tree/main}
      }</code></pre>
      </div>

    </section>
  </body>
</html>