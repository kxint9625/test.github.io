<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving"
    />
    <meta
      name="keywords"
      content="VLM, spatial reasoning, spatial understanding, Large Language Model, LLM, multimodal, visual language model, VLM, vision=language model, distance estimation"
    />
    <meta
      property="og:image"
      content="TODO"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
        NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving
    </title>

    <!--TWITTER TODO-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Spatial Visual Language Model" />
    <meta
      name="twitter:description"
      content="NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving"
    />
    <meta
      name="twitter:image"
      content="TODO"
    />

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="./static/images/favicon.svg" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <img 
                src="static/images/logo3.png"
                class="center"
                width="180"
              />
              <h1 class="title is-1 publication-title">
                NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark
                for Vision-Language Models in Autonomous Driving
              </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  Kexin Tian</a><sup>1</sup>,
                </span>      
                <span class="author-block">
                  Jingrui Mao</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  Yunlong Zhang</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  Jiwan Jiang</a
                  ><sup>2</sup>,</span
                >
                <span class="author-block">
                  Yang Zhou</a
                  ><sup>1,&#8225;</sup>,
                </span>
                <span class="author-block">
                  <!-- <a href="https://vztu.github.io/">Zhengzhong Tu</a> -->
                  Zhengzhong Tu
                  <sup>1,&#8225;</sup>
                </span>
              </div>
              <div>
                <sup>&#8225;</sup> Corresponding authors.
              </div>

              <br />
              <div class="is-size-5 publication-authors">
                <ul class="affiliation-list">
                  <li class="affiliation">
                    <img
                      src="./static/images/tamu.jpg"
                      alt="TAMU Logo"
                      class="logo"
                    />
                    <span class="author-block"
                      ><sup>1</sup>Texas A&M University</span
                    >
                  </li>
                  <li class="affiliation">
                    <img
                      src="./static/images/uwmadison.png"
                      alt="UW-MADISON Logo"
                      class="logo"
                    />
                    <span class="author-block"><sup>2</sup>University of Wisconsin-Madison</span>
                  </li>
                </ul>
                <br>
              </div>
              <br />
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a
                      href="[TODO]"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <!-- Video Link. -->
                  <span class="link-block">
                    <a
                      href="[TODO]"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://huggingface.co/datasets/ktian6/NuScenes-SpatialQA"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Benchmark</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://github.com/taco-group/NuScenes-SpatialQA"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
                Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning—key capabilities for autonomous driving—still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we propose <b>NuScenes-SpatialQA</b>, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMs' performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning.
              <!--/ Abstract. -->
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <div class="hr"></div>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">QA Generation Pipeline</h2>
            <div class="content has-text-justified">
              <p>
                Our QA generation framework involves two key pipelines: 
                <strong>①<u>3D Scene Graph Generation</u></strong>: We construct a 3D scene graph for each camera view in the NuScenes dataset. An <u>auto-captioning</u> process is designed to generate instance-level descriptions for each annotated object, and the generated captions are combined with selected 3D attributes from NuScenes as node attributes. Spatial relationships between objects are encoded as edge attributes.       
                <strong>②<u>Q&A Pairs Generation</u></strong>: Based on the structured scene graph and our predefined QA templates, we generate multi-aspect QA pairs that comprehensively cover both spatial understanding and spatial reasoning, providing a holistic evaluation of the spatial capabilities of VLMs.
                An overview of each step is presented in the following figure.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/framework.png"
                class="inline-figure-six"
                alt="QA Generation Framework"
                style="width: 88%; height: auto;"
              />
            </div>
            <h3 class="title is-4">QA Example</h3>
            <div class="content has-text-justified">
              <p>
                The final QA pairs are designed to comprehensively evaluate the spatial capabilities of vision-language models, and are categorized into two main types:
              </p>

              <ol>
                <li>
                  <b>Spatial Understanding</b>: Assesses direct spatial relationships and metric measurements, including:
                  <ul>
                    <li><u><em>Qualitative QA</em></u>: Tasks that evaluate relative spatial relations, like relative spatial relationships and dimension comparison.</li>
                    <li><u><em>Quantitative QA</em></u>: Tasks that involve direct numerical estimation, requiring models to extract specific values such as distances, dimensions, or angles.</li>
                  </ul>
                </li>
                <li>
                  <b>Spatial Reasoning</b>: Involves higher-level inference beyond direct attribute retrieval, including:
                  <ul>
                    <li><u><em>Direct Reasoning QA</em></u>: Deductive questions based on object relations.</li>
                    <li><u><em>Situational Reasoning QA</em></u>: Real-world scenario-based reasoning involving safety or physical constraints.</li>
                  </ul>
                </li>
              </ol>
                            
              <p>
                The figure below shows some examples of the QA pairs:
              </p>
              
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/example.png"
                class="inline-figure-six"
                alt="QA Example."
              />
            </div>
            <h3 class="title is-4">Benchmark Statistics</h3>
            <div class="content has-text-justified">
              <p>
                The final benchmark consists of approximately <strong>3.5 million</strong> QA pairs, including around <strong>2.5M</strong> qualitative and <strong>0.6M</strong> quantitative questions under the spatial understanding category, as well as <strong>0.2M</strong> reasoning-based questions covering both direct and situational reasoning.
              </p>
              
              <p>
                These QA pairs span <strong>6,000</strong> keyframes, each captured from <strong>6 camera views</strong> in the NuScenes dataset.
              </p>
              
              <p>
                As shown in the following figure, we compare NuScenes-SpatialQA with existing open-source benchmarks in autonomous driving and spatial reasoning to highlight its scale and coverage. It is the first <u>large-scale</u>, <u>ground-truth-based</u> QA benchmark specifically designed to evaluate both <u>spatial understanding</u> and <u>spatial reasoning</u> capabilities of VLMs in <u>autonomous driving</u>.
              </p>
              
            </div>

            <div class="content has-text-centered"> 
              <img
                src="./static/images/compare_table.png"
                class="inline-figure-six"
                alt="Benchmark Comparison Table."
                style="width: 98%; max-width: 900px; height: auto;"
              />
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Benchmark Evaluations</h2>
            <!-- <h3 class="title is-4">Spatial Understanding</h3> -->
            <figure style="text-align: center;">
              <img src="./static/images/table1.png" alt="Benchmark results on spatial understanding tasks." style="max-width: 100%; height: auto;" />
              <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
                Performance on <strong>spatial understanding</strong> tasks in NuScenes-SpatialQA. The upper part of the table reports results on <strong>Qualitative Spatial QA</strong>, where values represent <em>accuracy</em> (<strong>↑</strong>). The lower part presents results on <strong>Quantitative Spatial QA</strong>, where values correspond to <em title="A prediction is considered correct if it falls within a predefined numeric threshold.">Tolerance-based Accuracy *</em> (<strong>↑</strong>) / <em>MAE</em> (<strong>↓</strong>). Baseline marked with <strong>✧</strong> is spatial-enhanced VLM.    
                <br />
                <em style="font-size: 0.8em;">* Tolerance-based Accuracy is defined to measure the proportion of model responses that fall within the range of [75%, 125%] of the ground-truth answer.      </em>
              </figcaption>
            </figure>

            <div style="height: 36px;"></div>

            
            <figure style="text-align: center;">
              <img src="./static/images/radar.png" alt="Radar figure on spatial understanding tasks." style="max-width: 78%; height: auto;" />
              <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
                Radar figure of the <strong>spatial understanding</strong> evaluations.         
              </figcaption>
            </figure>
            <div style="height: 36px;"></div>

            <!-- <h3 class="title is-4">Spatial Reasoning</h3> -->
            <figure style="text-align: center;">
              <img src="./static/images/table2.png" alt="Benchmark results on spatial reasoning tasks." style="max-width: 88%; height: auto;" />
              <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
                Performance on <strong>Spatial Reasoning</strong> tasks in NuScenes-SpatialQA. The table reports <em>Tolerance-based Accuracy</em> (<strong>↑</strong>) across different VLMs.         
              </figcaption>
            </figure>
            <div style="height: 36px;"></div>

            <!-- <h3 class="title is-4">Effect of Backbone Architecture</h3> -->
            <figure style="text-align: center;">
              <img src="./static/images/table3.png" alt="Ablation study on backbone and scaling." style="max-width: 88%; height: auto;" />
              <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
                  Effect of <strong>backbone architecture</strong> and <strong>model scaling</strong> on VLM performance. This table reports <em>Tolerance-based Accuracy</em> (<strong>↑</strong>) across different model variants of LLaVA-v1.6. The first two rows compare the impact of different backbone architectures (Mistral-7B vs. Vicuna-7B). The last three rows examine the effect of model scaling.  
              </figcaption>
            </figure>
            <div style="height: 36px;"></div>

            <figure style="text-align: center;">
              <img src="./static/images/table4.png" alt="Ablation study on CoT." style="max-width: 49%; height: auto;" />
              <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
                Effects of <b>CoT reasoning</b> on VLM performance in NuScenes-SpatialQA.         
              </figcaption>
            </figure>

            <!-- <h3 class="title is-4">Effect of Chain-of-Thought (CoT) Reasoning</h3> -->
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Acknowledgement</h2>
            <div class="content has-text-justified">
              <p>
                The authors would like to thank Prof. Cheng Zhang for his valuable guidance during the early stage of this work.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title has-text-centered">BibTeX</h2>
        <pre><code>TODO</code></pre>

<p> If you are using the open source implementation listed above, please also cite:</p>

      <pre><code>TODO</code></pre>
      </div>

    </section>
  </body>
</html>